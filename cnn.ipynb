{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgJJSMzz9Z9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from baseline_cnn import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2zG_gzb9nXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNLkDijm9oJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check if your system supports CUDA\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# Setup GPU optimization if CUDA is supported\n",
        "if use_cuda:\n",
        "    computing_device = torch.device(\"cuda\")\n",
        "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
        "    print(\"CUDA is supported\")\n",
        "else: # Otherwise, train on the CPU\n",
        "    computing_device = torch.device(\"cpu\")\n",
        "    extras = False\n",
        "    print(\"CUDA NOT supported\")\n",
        "    \n",
        "criterion = __\n",
        "\n",
        "#TODO - loss criteria are defined in the torch.nn package\n",
        "\n",
        "#TODO: Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
        "optimizer = optim.Adam(net.parameters(),lr = __)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUUyS8AQ9r0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the generator\n",
        "net=Nnet().to(computing_device)\n",
        "net.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A--k99Kl9vDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "import torch.nn.init as torch_init\n",
        "import torch.optim as optim\n",
        "\n",
        "# Data utils and dataloader\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Track the loss across training\n",
        "total_loss = []\n",
        "avg_minibatch_loss = []\n",
        "N = 50\n",
        "\n",
        "for epoch in range(50):\n",
        "    N_minibatch_loss = 0.0\n",
        "\n",
        "    # Get the next minibatch of images, labels for training\n",
        "    for minibatch_count, (images, labels) in enumerate(data_loader, 0):\n",
        "        print(\"mini_batch\", minibatch_count)\n",
        "        # Zero out the stored gradient (buffer) from the previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
        "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
        "        # Perform the forward pass through the network and compute the loss\n",
        "        outputs = net(images)\n",
        "        labels= func.one_hot(labels, num_classes=201).type(torch.FloatTensor)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Automagically compute the gradients and backpropagate the loss through the network\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()    \n",
        "        # Add this iteration's loss to the total_loss\n",
        "        total_loss.append(loss.item())\n",
        "        N_minibatch_loss += loss\n",
        "               \n",
        "        \n",
        "        if minibatch_count % N == 49:\n",
        "            #Print the loss averaged over the last N mini-batches\n",
        "            N_minibatch_loss /= N\n",
        "            print('Epoch %d, average minibatch %d loss: %.3f' % (epoch + 1, minibatch_count+1, N_minibatch_loss))\n",
        "            # Add the averaged loss over N minibatches and reset the counter\n",
        "            avg_minibatch_loss.append(N_minibatch_loss)\n",
        "            N_minibatch_loss = 0.0\n",
        "\n",
        "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
        "    # TODO: Implement validation #with torch.no_grad():"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}